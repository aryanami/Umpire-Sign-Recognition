{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e920d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for image processing, neural networks, and handling images\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Initialize background and set parameters for ROI (Region of Interest)\n",
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "# Defining the Region of Interest dimensions\n",
    "ROI_top = 10\n",
    "ROI_bottom = 550\n",
    "ROI_right = 10\n",
    "ROI_left = 800\n",
    "\n",
    "# Function to calculate accumulated average for background subtraction\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee94d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to segment the hand region from the background\n",
    "def segment_hand(frame, threshold=25):\n",
    "    \n",
    "    global background\n",
    "    \n",
    "    # Calculate the absolute difference between the background and the current frame\n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "\n",
    "    # Apply threshold to get a binary image\n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours in the thresholded image\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        return (thresholded, hand_segment_max_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b8995f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start capturing video from the webcam\n",
    "cam = cv2.VideoCapture(0)\n",
    "cam.set(3,900)\n",
    "cam.set(4,1200)\n",
    "num_frames = 0\n",
    "element = 5\n",
    "num_imgs_taken = 0\n",
    "\n",
    "# Start an infinite loop to process each frame\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # Flip the frame horizontally\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "\n",
    "    # Convert the ROI to grayscale and blur it\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "    # Calculate the accumulated average for background subtraction after 60 frames\n",
    "    if num_frames < 60:\n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        if num_frames <= 59:\n",
    "            \n",
    "            cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "            #cv2.imshow(\"Sign Detection\",frame_copy)\n",
    "         \n",
    "    #Time to configure the hand specifically into the ROI\n",
    "    elif num_frames <= 300: \n",
    "\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"Adjust hand...Gesture for\" + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "        \n",
    "        # Checking if hand is actually detected by counting number of contours detected\n",
    "        if hand is not None:\n",
    "            \n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Draw contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "            # Also display the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        # Segmenting the hand region...\n",
    "        hand = segment_hand(gray_frame)\n",
    "        \n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "            \n",
    "            # unpack the thresholded img and the max_contour...\n",
    "            thresholded, hand_segment = hand\n",
    "\n",
    "            # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "            \n",
    "            cv2.putText(frame_copy, str(num_frames), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            #cv2.putText(frame_copy, str(num_frames)+\"For\" + str(element), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.putText(frame_copy, str(num_imgs_taken) + 'images' +\"For\" + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "            # Displaying the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "            if num_imgs_taken <= 300:\n",
    "                cv2.imwrite(r\"C:\\\\Users\\\\91773\\\\Mini Project\\\\train\\\\\"+str(element)+\"\\\\\" + str(num_imgs_taken+300) + '.jpg', thresholded)\n",
    "                #cv2.imwrite(r\"D:\\\\gesture\\\\x\"+\"\\\\\" + str(num_imgs_taken) + '.jpg', thresholded)\n",
    "            else:\n",
    "                break\n",
    "            num_imgs_taken +=1\n",
    "        else:\n",
    "            cv2.putText(frame_copy, 'No hand detected...', (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "    # Drawing ROI on frame copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)\n",
    "    \n",
    "    cv2.putText(frame_copy, \"DataFlair hand sign recognition_ _ _\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    \n",
    "    # increment the number of frames for tracking\n",
    "    num_frames += 1\n",
    "\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "    # Break the loop when the ESC key is pressed \n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Release the camera & close all windows\n",
    "cv2.destroyAllWindows()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fffda2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 5 classes.\n",
      "Found 305 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = r'C:\\Users\\91773\\Mini Project\\train'\n",
    "test_path = r'C:\\Users\\91773\\Mini Project\\test'\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d12eb9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc52197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = next(train_batches)\n",
    "\n",
    "\n",
    "#Plotting the images\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9273d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Using cached scipy-1.8.0-cp310-cp310-win_amd64.whl (37.0 MB)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in c:\\users\\91773\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scipy) (1.22.3)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "object-detection 0.1 requires apache-beam, which is not installed.\n",
      "object-detection 0.1 requires avro-python3, which is not installed.\n",
      "object-detection 0.1 requires contextlib2, which is not installed.\n",
      "object-detection 0.1 requires Cython, which is not installed.\n",
      "object-detection 0.1 requires lvis, which is not installed.\n",
      "object-detection 0.1 requires pycocotools, which is not installed.\n",
      "object-detection 0.1 requires tensorflow_io, which is not installed.\n",
      "object-detection 0.1 requires tf-models-official>=2.5.1, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c95d71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(5,activation =\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7c43053",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "672ab65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "120/120 [==============================] - 69s 388ms/step - loss: 1.8263 - accuracy: 0.9492 - val_loss: 0.5239 - val_accuracy: 0.8098 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 8s 63ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.5002 - val_accuracy: 0.8197 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 7s 59ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4788 - val_accuracy: 0.8295 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 8s 64ms/step - loss: 7.8010e-04 - accuracy: 1.0000 - val_loss: 0.4712 - val_accuracy: 0.8295 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 5.8311e-04 - accuracy: 1.0000 - val_loss: 0.4624 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 9s 76ms/step - loss: 4.6368e-04 - accuracy: 1.0000 - val_loss: 0.4537 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 3.8326e-04 - accuracy: 1.0000 - val_loss: 0.4473 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 8s 67ms/step - loss: 3.2600e-04 - accuracy: 1.0000 - val_loss: 0.4439 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 2.8320e-04 - accuracy: 1.0000 - val_loss: 0.4393 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 8s 67ms/step - loss: 2.4992e-04 - accuracy: 1.0000 - val_loss: 0.4369 - val_accuracy: 0.8393 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5983ba7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of 0.0014339596964418888; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = next(train_batches) \n",
    "\n",
    "imgs, labels = next(test_batches)\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "#model.save('best_model_dataflair.h5')\n",
    "model.save('best_model_dataflair3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1081719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [1.8263061046600342, 0.0022573661990463734, 0.0011674984125420451, 0.0007800954044796526, 0.0005831113667227328, 0.0004636790254153311, 0.00038326214416883886, 0.0003260004159528762, 0.00028320233104750514, 0.0002499150577932596], 'accuracy': [0.9491666555404663, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.523883581161499, 0.5002407431602478, 0.47880828380584717, 0.47119367122650146, 0.4623553156852722, 0.4536508023738861, 0.4472721219062805, 0.44385355710983276, 0.4392538070678711, 0.4368702471256256], 'val_accuracy': [0.8098360896110535, 0.8196721076965332, 0.8295081853866577, 0.8295081853866577, 0.8393442630767822, 0.8393442630767822, 0.8393442630767822, 0.8393442630767822, 0.8393442630767822, 0.8393442630767822], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]}\n"
     ]
    }
   ],
   "source": [
    "print(history2.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbbae88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of 0.5267590284347534; accuracy of 69.9999988079071%\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 31, 31, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 15, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                294976    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 413,701\n",
      "Trainable params: 413,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(test_batches)\n",
    "\n",
    "model = keras.models.load_model(r\"best_model_dataflair3.h5\")\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "scores #[loss, accuracy] on test data.\n",
    "model.metrics_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98281ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions on a small set of test data--\n",
      "\n",
      "wide   noball   wide   out   out   wide   wide   four   wide   wide   "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACGgAAADaCAYAAADw3eaaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAc5klEQVR4nO3d23YkqY4A0PIs//8vex569XFWtiNMAAIEez/11HFm3BAiY1jSx9fX1x8AAAAAAAAAAOL83+wTAAAAAAAAAADYnQ0aAAAAAAAAAADBbNAAAAAAAAAAAAhmgwYAAAAAAAAAQDAbNAAAAAAAAAAAgtmgAQAAAAAAAAAQ7PPuf/z4+PgadSKwg6+vr4+SvxNb8IzYghhiC2KILYghtiCG2IIYYgtiiC2IURJb4gqeuYorFTQAAAAAAAAAAILZoAEAAAAAAAAAEMwGDQAAAAAAAACAYDZoAAAAAAAAAAAE+5x9AgAw2tfX1//+++PjY+KZAAAAAAAAcAoVNAAAAAAAAAAAgtmgAQAAAAAAAAAQzAYNAAAAAAAAAIBgnz2+5Ovr63///fHx0eMrAQAAAMJ4lwEAAACMpoIGAAAAAAAAAEAwGzQAAAAAAAAAAIJ1aXEC9PFaYvfPH2V2AQAAAAAAAHahggYAAAAAAAAAQDAbNAAAAAAAAAAAgtmgAQAAAAAAAAAQzAYNAAAAAAAAAIBgNmgAAAAAAAAAAASzQQMAAAAAAAAAIJgNGgAAAAAAAAAAwWzQAAAAAAAAAAAI9tnjSz4+Pnp8DRxPLAEAAAAAAADsSQUNAAAAAAAAAIBgNmgAAAAAAAAAAASzQQMAAAAAAAAAINjn7BMAgNE+Pj5mnwIAAJNZEwIAAACjqaABAAAAAAAAABDMBg0AAAAAAAAAgGBanAAAR/n6+vrx35U5BwAAAAAAIqmgAQAAAAAAAAAQzAYNAAAAAAAAAIBgWpz8+bvU+Wt586sS6O92LYl+dV9qPvN+L3e9Z6drfc6nxxyUepq3xAw8J54ghtiCfrzL+Jl3GWRzeswCsL7SXPVqp7xlTUg068HzqKABAAAAAAAAABDMBg0AAAAAAAAAgGBanPy5Lglz9e/vpWZqymfCCZ7GxvvfXJV1EnOcrjVvlXzmdEoX7qVn3pCD4JvYgrm8y4A91LwLuft8Bn5vIQdxmuxj/u6cT8hbEM16MOc1tFBBAwAAAAAAAAAgmA0aAAAAAAAAAADBtDipcFdqJnupKuipNQZeP6/dCdSTt/5WMrecci920/N5Ps1BtceBDMQW7MGakJ3sPGZLrucqfks/v5qdn+fJ7samFq2wvtK5+cS8xRp2Xj+cGFc7P8+fqKABAAAAAAAAABDMBg0AAAAAAAAAgGA2aAAAAAAAAAAABPucfQI7KOmjnFFpj5+drpkyq/aC2qHPFjGMjb/tmrdeeeZnGNWfuDRmVs2P8FTv3HAVG2IL5jphTXjnxGtmH9nj9z2fZ7wGflezbrv7jDUhjDXjvQpQLntcnb4eVEEDAAAAAAAAACCYDRoAAAAAAAAAAMG0OLlRUzZNeTWeUp6vTPZyTTCCvMVpepfCK4kh+YgTzCgzKbYY4ZQWaKevCXe6Fihxl7ezxEOW82Suq/Wi8QO57JC3YDU7xFWW8+xFBQ0AAAAAAAAAgGA2aAAAAAAAAAAABKtqcXJKWdBXd2V2r64/YwmZ2dyz3DwzWIe81Zd7s6aeLU3u/jfPnNPU5JDSz7fGltiEZ6wJx3DPoI0Y4injBBjJnAPxTlsPqqABAAAAAAAAABDMBg0AAAAAAAAAgGBVLU5OUVLu8/3/3rXsSu+2NifcM67L6ZbGVs/jG2ecQN5qU3Nf3Mt8anKNsQExxAnEGPV7K4O7dxmv/33ivYGVWAec4WrtZ03IrqwvAMqdtgZQQQMAAAAAAAAAIJgNGgAAAAAAAAAAwbQ4qfBeZuXEUlUlJek4T+vzL40tYw6ekbc4RUmZ3LtyeSWxcVd+V2xxgpIS1DWxIbYg3l1snVJOtvWaT7xnKzk9H5SOP60jWEVpy+zS93zGMORSk7fgN9aD1oO7UEEDAAAAAAAAACCYDRoAAAAAAAAAAMFs0AAAAAAAAAAACPY5+wTYi/5FjJCxz9jVOesDDXOV9rrlDLuOB7mFVq2xsWtsAfuQH8nAOCUbYxbOZg6A/sTVPlTQAAAAAAAAAAAIZoMGAAAAAAAAAEAwLU6oclVGp7SNA9+y3JvW8uhZrnO00jLfytMD/ONu3jRXQj2xBZyg57sMLZuYabfcvNv1UM/zh7NlyQdZzpO97TYOd7ue36igAQAAAAAAAAAQzAYNAAAAAAAAAIBgR7Y4uStD2Vo2RYnLn73fl9f7fFrZmqw8mzJP54D3+2oO4Se989bVHLwTc9aZej73kjjZNX4gktiCetaE7Z6+f7h7l1Hy78xxwnum3a7Le0IAoKcT1hO7Xddp60EVNAAAAAAAAAAAgtmgAQAAAAAAAAAQrKrFSfZyInctBa7KppxS7vPK3T2rGQ+n389TtM4VGWJw1fNiL/JWO/djT73XJ8A/tGArY85hNGvC+dzP8dzz35W24pmtJm/KtcAu7uaw0+a6LHnrztU5n/YsR7Ee/F2WuLIe/KaCBgAAAAAAAABAMBs0AAAAAAAAAACCVbU42c1V+c/WUisr27UkDHOVlmor/cyqSuOnZD4onTMy3qeVZL9/dyXKTslbPZWU/T7xvmTRGs81caJUPCcQW5zAmrD8u1f19Nq0bMrvapzfPf+az8yUpSR1jdLfWztd84kyxBmspHRNt2o8tc7hq17Xnz/lbQP9lh3LejA368GfqaABAAAAAAAAABDMBg0AAAAAAAAAgGA2aAAAAAAAAAAABPucfQK9jeglVNoXZ+V+OaP7L+n7ml/JOLmLjdaxVXrMlaw8B7COkthqHUs75C34V+m6pWZ9k6EnJUQRW7C+1t/VO6wJvcuA/1otZkvipLRnPEAGT9ck5rm1XOWt0ufqebKC1cah9eDvVNAAAAAAAAAAAAhmgwYAAAAAAAAAQLDtWpxElbs8vSzvXTkaJT75V+9y2CXftbOr6zxxDtrZiLy1m9k5eed7u5OSsu2lpd6BZ8QWtKkpp1xSmvlE3mXAeCVz07vZv/Fo45nB/qyb/naVt+Qz+If14O9U0AAAAAAAAAAACGaDBgAAAAAAAABAsOIWJxlLi7SeZ00JloxGPM+d798pSsbJXTnrnuMsS2yufG4nkLfyXf9K3u+ZlkNrKi1bfmIOghY1Y/suNkriUWwRJeOayJqwTM31PJ1rzEdzlIzbncZ29vPv4YTnDJwt+3yW8ZxL1fwW3fl+rMJ68DwnPOdXKmgAAAAAAAAAAASzQQMAAAAAAAAAIFhxi5Odyoa861myMvt9Ki0VowQx/OOurUvJv//2fdTbeT4uGSel15/xPo0o2y0Wc6iJjTvZS+ZdnX/Ga2Gu3rEFM50+B+68JnyVPYdTpvRdlLy1D/HMK3M92chbz2WP7eznn4G4Os+ucaWCBgAAAAAAAABAMBs0AAAAAAAAAACC3bY4OaVs2FVJHCVwOF2GOeCupNWMcy6ZN8wtcTKM2VI1baauxlb2e1FDiTuu3D3/p7Fy1wbixLjjbDvHlvZB+cweM7NZE8K37OO+dT5b9fpLf5ONaG/JWu6es+fOCbKP813n6pq8xTqyPxfrwT3XgypoAAAAAAAAAAAEs0EDAAAAAAAAACCYDRoAAAAAAAAAAME+Z5/AarL3rOmptP8P/OuU+LnqeXWnNZ5Oubc8Z2z87O6+yG9nuBoDNXP4qrKfPzmdEFuQkTXhN3PQXq76TBvzZxDP57lbU4p7MpC3znaVtzz/NuLqbLuuB1XQAAAAAAAAAAAIZoMGAAAAAAAAAECwrVucXJW9Kf3Mq1NK5ShNzJWaeOJ32jDwSt567ipvnXL9nMc4B9ifNeFzNe8yvP/Y107jfqdrgaeMf06x01jf6VrIbaexuNO18E0FDQAAAAAAAACAYDZoAAAAAAAAAAAEu21xkr1sSs35X31GOem/Xd2PmrKg7mc+q5bPnXH892M+jQGldPuaPQZbyVv9vMfW6z1Qznofd8+vZK1SExszxoxxyminx9b7+Z+YR7PL/sysCeP0vB/uJwAAADVU0AAAAAAAAAAACGaDBgAAAAAAAABAsNsWJye6Knd5YulKJeDJ5q6lAexK3iqjvPd5otYxs9dHpePXmCdK1HiaHVuwqxNzgPkkv5Jxa62Tj9gEdiVv7Unemktc7UlcfVNBAwAAAAAAAAAgmA0aAAAAAAAAAADBjmlxUlrqRhkc+Jat3NAK8ZvtnrEuees58cerqzHQM2ZGxV9NWxOIslNs1RxTCVVGM+bgZ+JhXdakAGQib+VlPbgucfU7FTQAAAAAAAAAAILZoAEAAAAAAAAAEMwGDQAAAAAAAACAYJ+zT2A1Uf1ds/eNfT/n1+u5ura7a9Z/iBNcxUON7HMIca7GVus4MeauuR9z3c2tpesQ4L9qYusUJ14z+VgT/qz3uwygnveEwMmsKXKTt6AP68FvKmgAAAAAAAAAAASzQQMAAAAAAAAAINgxLU5qSkj1LGu5WwmrkjLHu13z6U4vbQ2jzZ5DZx+/ldZcXGld380ue14zFmefM/vq2RKgdA42hjnN7DE/+/i9tb7LMB/Bz6J+L3kXxU7kEFhHSU6piVl5i5NZDz6jggYAAAAAAAAAQDAbNAAAAAAAAAAAgm3d4qS1BFHJ9z757pLv27W82U5lZ7jWOzZ2smsZJvrKkrcyai1nfeI9y+hpDNXERsaxkPGcWcvs2Bo9hkvXaif8jmOOLGvC7G3DSvjtNkeGscF4d7/pjBOyMc/txfPkJ8ZCG3HFT3ZaD6qgAQAAAAAAAAAQzAYNAAAAAAAAAIBgW7c46emurGVNqfSavyvRei6wEmWsIEZrrugZm1niPMt5nub9Wbw+p6tnpuUU/K53bK06h5oPWNWoOJn9/qC1BR0AAGSQvRUD0J8KGgAAAAAAAAAAwWzQAAAAAAAAAAAIZoMGAAAAAAAAAECwz9knEEkfp296XNFTzfip6T1unHKanmP+qo93D1GxWTNPRIm8f8R5Om5mj7Mar+d8N06vrs2akBonxFaNU66T3DKuCV/JW9CmdQ4QcwCMJG9Bf+JqTSpoAAAAAAAAAAAEs0EDAAAAAAAAACDY1i1OZutZSrSmJYTy7KxEGaRvrXG6UhsI1lUzzlYqd6acNSVKx4l589vp10+Z02Pr/VqurrOmzRBkMGNN6F3GeVbKIa2/PbL8drk7r6u4u/r39+8Sw8DuVspbrTLmrdZ3m/LWmlaKK+tB68HRVNAAAAAAAAAAAAhmgwYAAAAAAAAAQDAtTjZTU+pp1VI7AORSmoNq8o68dYbSknm//Xvt38GuxNYzkfkMThC5brMm3NPdvBv1zI3Tvz0tSd27DDjsyljf04y8dbqatgx3fyNvrcd6cD7rwbFU0AAAAAAAAAAACGaDBgAAAAAAAABAMC1OAl2Vg3kv81JTKubqOE9L0Px2brCi0jH7tJz3DNnPnxxK805p3ir5LrhSuqahjfXdecRWm5rfUXCa1ncZygbDeDWxcfWZ0tLjNVrzsPkA2N0pa6oseQsyyRJXp60HVdAAAAAAAAAAAAhmgwYAAAAAAAAAQLAlWpxkKzvSm3K6wBOnz5krOP0Z9MxVWnOdIcszy3Ke/7orNZ/h/GmX5ZlnOU94atexXbo+e7qOe/+bne4ZZa5iZtS7MO/c2kTNDa3nEnmc7My70I+8FSdqTR2Zt05rxdCT9WBu1oP9qKABAAAAAAAAABDMBg0AAAAAAAAAgGA2aAAAAAAAAAAABPucfQJ//uTosdTaL+fuGlv74vTsVzWi39f7cWhT2udpJ6XXNaOHWZSra9FPdI5d7/OoHoAZ8hZ9ZXk2Wc6zRE3fVPLJ8vyynGcJeYdXGcZAzZpudl/oGmKTp0a985oxNmfHw8pzxQlmP3/YyUrx5P9XE+f06z+Z9eCY4/MzFTQAAAAAAAAAAILZoAEAAAAAAAAAEGxKi5OMJflryrHUlAXNUj6UNWWIJcqZD9aRMW+Nsus4Ve6x3exSeicqiUfPIj+xFU8O4ErGNeGo8duzlcpKzAdjRLYFJsbs2DAugJl2zlsZ1mc1euet7M95RTvH1a6sB9uooAEAAAAAAAAAEMwGDQAAAAAAAACAYFNanIzSsxxTaXmdnUq971rOagelY+7pc8tYsneU2fGw09zCtRnj7PSxNTu2AQDeeZfRpub+WROeYfZvrKhxtsO7nOzzzkoyPv+d7ZRfGW923oqyQ94ir9lxZT147bQ8qYIGAAAAAAAAAEAwGzQAAAAAAAAAAIJNaXFyYonJ0tIsrSVcokqU7lAeZ1ejyt9mt9O9UR5xvN3yVsm5ZcxbNfd/5eeUkTlpTcZ5fmJrLmsv/rXbmrBExjXh1fe+f3fJv5NThhj0LqeMlkVlRl3zife2J2tKrqwaW1HntdI1rmDV559dhvtqPVhm1/WgChoAAAAAAAAAAMFs0AAAAAAAAAAACDasxUlr2fEZny9V8n01JcxWLnvWej6rlpRZWVTJ11NkKGnEWnbOWy3n0vszUXq35jJvlIkaA7Pj4dVK51Iq4znzN7G17nezv53XhN5lPGcOaRN1/1YbZ69K4qH0t8vV51cYlyPifuXnPNuoMbDCWNuFe5mDvPX143/X3JfVfpOtvF7dnbiyHmyx8nN+SgUNAAAAAAAAAIBgNmgAAAAAAAAAAASzQQMAAAAAAAAAINjn7BMolaV3fGn/oCs9e/Tc9fXaqU/P6VboLXWC1frksb7seatUlrwl78Uo7Y/YaqV5d6VzuaOfam5iK8d3Wx9SIvua0LuMv4n751bukz1aaS/xknF+4v3jucg15ez5MOr4s6+LtZw+HkrzVst37Wb2WnVV1oPfrAf5lwoaAAAAAAAAAADBbNAAAAAAAAAAAAg2rMXJ7FIrd+VgRpzb3TFOLHt0enmwUiVlZncqI7jy8bOM09nPZiez79/svHV3vBPzFusYNc+NOM5KcQ5iC342e8zMHs/eZfxt9njIzv372W73pWfLI237rs1+/zN73GprQhTj4Xe73ZeovPVKDvu22/jpZbf7Yj34OxU0AAAAAAAAAACC2aABAAAAAAAAABBsWIuTUa5KUJWW5WwtI1NSaqW0BEtkOaTZJWF2K9czgnvWT2nMZyhpp/VEfvIWmd2Nv1XnzVIzysaLFf41I7Z2ahsntljV+1i8WodlXxM+/V6oMbv9zygZ3kvUqMnV5pO/lYyHUWNm11Z52a+FtZySt3blN+aaTomrXXPG6etBFTQAAAAAAAAAAILZoAEAAAAAAAAAEGy5FidXpVpKS7isVN5lpXOBnmpis9WMMk4lx2lt/UB+8tYYO5Uvy252Wb27sTD6fHoff+UYJN7s2Ho1O7Z6y37+5HbimjDyWqwJ99Sad1aKH67NeE4rra96aL2envdj15bZK71zZF3y1rWd5l15ayxxdQZx9U0FDQAAAAAAAACAYDZoAAAAAAAAAAAEW67FyVV5kRll10rLnlx9d+k5jyipovQnp4uKs/fvXbVcEnHkrRjyFiVmz7O9jz8ih+zWuoIYu40LscVM1oRtrAn3dfds/a7+lv36s5//bno+j6vvmhG/Ja1n373+3UrjVCvldclbZbJff/bzz0Zclcl+/dnPP4oKGgAAAAAAAAAAwWzQAAAAAAAAAAAIZoMGAAAAAAAAAECwz9kn8C6qr1BN39Wa7776rpoesPrynCdLf91R4zRjPMw+z9nHp59ReavkGPIWr7LM+xnHZpbzJMaMHt2lx6/5zEqynCe5eZeRL++yj/fxawyuo3Ru8MzKjLhPd7mlJG/cfVfP33gtf/Pk71pd3bOn9/KdvJubvLUueSsvcbUucfUzFTQAAAAAAAAAAILZoAEAAAAAAAAAEGy5FiclZb9ardxGouQ63/9m5euhzaolfUbEaRZKZ/FqVN5qLYV59d015yxv0dOokvCrOj2nMkbUXL+ynrHVMwezlxnvMlbKm9aEvBrRkvHu709vm9fT6dc/W+S839pWY9QxsytpIdbzGOJ0Dvcd/st6cB+nX38UFTQAAAAAAAAAAILZoAEAAAAAAAAAEGy5Ficj3JVguSrVUlrCZaXyLqXXCb2c2O5j5WtUemofpc8ve9l6ZePXZT4Zw709T2tsic0y7g2jRY25mncZs9Wcl3cZZ2hd+999ftcy1jXnNeNaxGk/K40/cjjx3WwPJfNWZN7yu/DbjHshb81jPfic9eA+VNAAAAAAAAAAAAhmgwYAAAAAAAAAQLDlWpzMLrVydcyVStj0oIw8vUSWZ9st7kZz/8ZYNW9lp7R1fieMU5hBbMGaesZm6XedOB94l5FPydhsHb/vn9/1XcSMa1n1XgBEGTHv3eWtHt+Xmby1J+vBfqwH96GCBgAAAAAAAABAMBs0AAAAAAAAAACCLdfi5NUp5TpLrrP3vbj6PiVCadFafvd9/O0c9+xJ3nr2N5zjan1hbEAbsQV7yhLDK7X68y5jLSPGwym/N2Zf567lwTGHQoue7exWMzvvtJK31mE92M/s6xRXMVTQAAAAAAAAAAAIZoMGAAAAAAAAAEAwGzQAAAAAAAAAAIJ9zj6BFVz1v6npbXr1XXc9dp7233k/fk3PwJLzXLkX2k52vs8l1za7fxb0NGoMt+atkvPsmbfuPl8zB8pV4+kvCDHEFtAi25rQuwx4bnbe32mt4v3TtZJ8AjsbMSfczUErxVr2+VHeYkezn7+4iqGCBgAAAAAAAABAMBs0AAAAAAAAAACCaXFSobTsSc9SKTPKdSoROsbO97m1ZC5ks1KJrFer5a2e92bnOZR+epZ6B76JLVhTTevVq3WUdxmsKrIl40pW/Y15Z9VzXulcsmjNG+b98YzzdZ2St+jHuPjdKXG16trqzqrnvNK5qKABAAAAAAAAABDMBg0AAAAAAAAAgGBanPy5LrVSU+qkpMzv+/fOLvVScvy7c6bN7OcP5CNv/X79pXlq9rXsbPa9nX18WFVrbIgt+N2oOBlxnIxrwlfeZZxh59ZaWa7laWzt/MxOUdPWuOccXDLv7zbni5N9mAPne9reb9QzMxbq7RxXWa7FevAZFTQAAAAAAAAAAILZoAEAAAAAAAAAEOzIFieRZVNKy2pG2e04MNLpJZVYV+nYrCkzvWveEr9zRJU67/m9s8uxz7Bzmd9TPC2/eqe0ndVTJ8YWXBnRbqT3cWbHbfa1J21qxvYpa5pV82vv+1/yfa3tPZlv9HMrPd7K84mxviZ5Kx95a33i6pr14Led4koFDQAAAAAAAACAYDZoAAAAAAAAAAAE697ipGeplRllz3uWD356jB7fl72ky4k8szhig9F2bdfRu5y22DxD1JqwdB2VfWyJE0YQW/u0h+AMs8eZdxnUiGqnNeqYs2U//9ZnUbpWyX6fmKukjSzUODFvZb8WeSuG9WCb7Ocvrn6mggYAAAAAAAAAQDAbNAAAAAAAAAAAgtmgAQAAAAAAAAAQ7LP3F2bo8fLeo+b1nEt62dx9vsT73z/tZ9f6edayUy+sSDX3qTU2YRWteScyb5V8l7x1pqg5tbTv4KuMuTbLeTKe2GqT5TzhJ3druqu4vfsb7zIYoee8e0qu2snVPe89n0CEu3FWk8OM2/OcmLcynvMreSuG9eDZxNXPVNAAAAAAAAAAAAhmgwYAAAAAAAAAQLDuLU5WdVfC5up/Ky27UnLMu8+3lmG5+nxp2Z7sZWCyc//LuE+cZue8VXJ8MZ/TiOdmnHAisbUm94kRataEV2rG7N0xvMtgVTWlr4lRE8+vn2l9fqW/cdlTaWuwmvFgDNGTvDXe07xztw7umbde+Y3eRlytw3rwZypoAAAAAAAAAAAEs0EDAAAAAAAAACDYh9IuAAAAAAAAAACxVNAAAAAAAAAAAAhmgwYAAAAAAAAAQDAbNAAAAAAAAAAAgtmgAQAAAAAAAAAQzAYNAAAAAAAAAIBgNmgAAAAAAAAAAAT7fwCefv4kmC0KAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual labels\n",
      "wide   noball   wide   out   out   wide   six   four   six   six   "
     ]
    }
   ],
   "source": [
    "word_dict = {0:'four',1:'noball',2:'out',3:'six',4:'wide'}\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)], end='   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3126e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
